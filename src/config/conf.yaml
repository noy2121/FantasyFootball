defaults:
  - _self_
  - override hydra/hydra_logging: disabled
  - override hydra/job_logging: disabled

hydra:
  output_subdir: null
  run:
    dir: .

mode: fine_tune

rag:
  rag_dir: 'data/rag'
  csvs_dir: 'data/csvs'
  estimation_data_dir: 'data/team_estimation_data'
  generate_jsons: False
  embedding_model_name: 'all-MiniLM-L6-v2'

data:
  bucket: 'deep-learning-projects'
  data_dir: 'data'
  train_filepath: 'preprocessed/train'
  test_filepath: 'preprocessed/test'
  process_raw_data: False
  create_text_data: False
  generate_data: True
  split_data: False
  start_year: 2018
  test_year: 2023

model:
  model_name: 'EleutherAI/pythia-1b'
  load_in_4bit: True
  use_flash_attention: True
  cache_dir: '.cache\huggingface'
  hf_token_filepath: 'data/keys/huggingface_token.txt'
  max_length: 4096

train:
  num_epochs: 3
  batch_size: 1
  peft_method: 'lora'
  evaluation_steps: 100
  learning_rate: 2e-4
  weight_decay: 0.001
  mixed_precision: False
  accumulation_steps: 16
  out_dir: 'results'

peft:
  r: 16
  lora_alpha: 32
  target_modules: ['query_key_value']
  dropout: 0.1

evaluate:

inference:
  vanilla: True


#################
# models
################
# 'bigscience/bloomz-560m', bigscience/bloomz-1b7, bigscience/bloomz-3b
# EleutherAI/gpt-neo-1.3B, EleutherAI/gpt-neo-2.7B,
# EleutherAI/pythia-410m, EleutherAI/pythia-1b, EleutherAI/pythia-1.4b, EleutherAI/pythia-2.8b
# facebook/opt-1.3b, facebook/opt-2.7b
# microsoft/Phi-3-mini-4k-instruct (3.8B), microsoft/Phi-3.5-mini-instruct (3.8B), microsoft/Phi-3-small-8k-instruct (7B)
# mistralai/Mistral-7B-Instruct-v0.3, mistralai/Mistral-7B-Instruct-v0.2, mistralai/Mistral-Nemo-Instruct-2407
