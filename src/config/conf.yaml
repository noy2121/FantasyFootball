data:
  bucket: 'deep-learning-projects'
  datasets: ['players', 'games']
  data_files_path: 'data'

model:
  model_name: 'bigscience/bloomz-560m'   # mistralai/Mistral-7B-Instruct-v0.2, mistralai/Mistral-7B-v0.1,
  load_in_4bit: True
  quant_type: 'nf4'
  quant_compute_dtype: 'float16'
  use_double_quant: False
  cache_dir: '.cache\huggingface'
  huggingface_token_filepath: 'data/keys/huggingface_token.txt'

peft:
  method: 'p-tuning'
  r: 64
  lora_a: 16
  target_modules: ['query_key_value']
  dropout: 0.1

train:
  num_epochs: 1
  batch_size: 4
  gradient_accumulation_steps: 1
  optim: 'paged_adamw_32bit'
  save_steps: 1000
  logging_steps: 1000
  learning_rate: 2e-4
  weight_decay: 0.001
  checkpoint_name: 'results'

evaluate:
  models_dir: 'results/ft_models/bigscience'
  out_dir: 'evaluations'