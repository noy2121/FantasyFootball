defaults:
  - _self_
  - override hydra/hydra_logging: disabled
  - override hydra/job_logging: disabled

hydra:
  output_subdir: null
  run:
    dir: .

data:
  bucket: 'deep-learning-projects'
  data_dir: 'data'
  process_raw_data: False
  create_text_data: False
  generate_data: True
  split_data: False
  start_year: 2018
  test_year: 2023

model:
  model_name: 'bigscience/bloomz-560m'   # mistralai/Mistral-7B-Instruct-v0.2, mistralai/Mistral-7B-v0.1,
  load_in_4bit: True
  quant_type: 'nf4'
  quant_compute_dtype: 'float16'
  use_double_quant: False
  cache_dir: '.cache\huggingface'
  huggingface_token_filepath: 'data/keys/huggingface_token.txt'

peft:
  method: 'p-tuning'
  r: 64
  lora_a: 16
  target_modules: ['query_key_value']
  dropout: 0.1

train:
  num_epochs: 1
  batch_size: 4
  gradient_accumulation_steps: 1
  optim: 'paged_adamw_32bit'
  save_steps: 1000
  logging_steps: 1000
  learning_rate: 2e-4
  weight_decay: 0.001
  checkpoint_name: 'results'

evaluate:
  models_dir: 'results/ft_models/bigscience'
  out_dir: 'evaluations'