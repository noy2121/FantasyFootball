defaults:
  - _self_
  - override hydra/hydra_logging: disabled
  - override hydra/job_logging: disabled

hydra:
  output_subdir: null
  run:
    dir: .

mode: fine_tune

rag:
  rag_dir: 'data/rag'
  csvs_dir: 'data/csvs'
  estimation_data_dir: 'data/team_estimation_data'
  generate_jsons: False
  embedding_model_name: 'all-MiniLM-L6-v2'

data:
  bucket: 'deep-learning-projects'
  data_dir: 'data'
  train_filepath: 'preprocessed/train'
  test_filepath: 'preprocessed/test'
  process_raw_data: False
  create_text_data: False
  generate_data: True
  split_data: False
  start_year: 2018
  test_year: 2023

model:
  model_name: 'bigscience/bloomz-560m'   # mistralai/Mistral-7B-Instruct-v0.2, mistralai/Mistral-7B-v0.1, or local
  load_in_4bit: True
  quant_type: 'nf4'
  quant_compute_dtype: 'float16'
  use_double_quant: False
  cache_dir: '.cache\huggingface'
  hf_token_filepath: 'data/keys/huggingface_token.txt'
  max_length: 4096
  gradient_checkpointing: True

train:
  num_epochs: 10
  batch_size: 8
  peft_method: 'all'
  evaluation_steps: 100
  learning_rate: 2e-4
  weight_decay: 0.001
  mixed_precision: True
  accumulation_steps: 1
  out_dir: 'results'

peft:
  method: 'lora'
  r: 16
  lora_alpha: 32
  target_modules: ['query_key_value']
  dropout: 0.1

evaluate:
  models_dir: 'results/ft_models/bigscience'
  out_dir: 'evaluations'

inference:
  vanilla: False