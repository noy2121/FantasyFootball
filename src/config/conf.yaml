data:
  bucket: 'deep-learning-projects'
  datasets: ['players', 'games']
  json_files_path: 'FootballGPT/data/jsons'

model:
  model_name: 'mistralai/Mistral-7B-v0.1'  # bigscience/bloomz-560m
  load_in_4bit: True
  quant_type: 'nf4'
  quant_compute_dtype: 'float16'
  use_double_quant: False
  huggingface_token_path: None

peft:
  method: 'lora'
  r: 64
  lora_a: 16
  target_modules: ['query_key_value']
  dropout: 0.1

train:
  num_epochs: 3
  batch_size: 4
  gradient_accumulation_steps: 1
  optim: 'paged_adamw_32bit'
  save_steps: 1000
  logging_steps: 1000
  learning_rate: 2e-4
  weight_decay: 0.001
  checkpoint_name: 'results'