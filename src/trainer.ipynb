{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33226d7b-4f61-4153-b56a-8b33a91d261d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-28 15:30:21.054022: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "from sagemaker import hyperparameters\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from pathlib import Path\n",
    "from peft import LoraConfig, PrefixTuningConfig, AdaLoraConfig, LoKrConfig\n",
    "\n",
    "from utils.utils import set_random_seed, get_root_dir\n",
    "from model.data_preprocess.football_torch_dataset import FootballTorchDataset\n",
    "\n",
    "set_random_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d0ee1a2-203f-46bf-a493-73db95bac376",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2679/1915747848.py:4: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  with initialize(config_path='config'):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:\n",
      "  datasets:\n",
      "  - player_valuations\n",
      "  bucket: deep-learning-projects\n",
      "  data_folder: sagemaker/FootballGPT_Data\n",
      "model:\n",
      "  model_name: bigscience/bloomz-560m\n",
      "  load_in_4bit: true\n",
      "  quant_type: nf4\n",
      "  quant_compute_dtype: float16\n",
      "  use_double_quant: false\n",
      "peft:\n",
      "  method: lora\n",
      "  r: 64\n",
      "  lora_a: 16\n",
      "  target_modules:\n",
      "  - query_key_value\n",
      "  dropout: 0.1\n",
      "train:\n",
      "  num_epochs: 3\n",
      "  batch_size: 4\n",
      "  gradient_accumulation_steps: 1\n",
      "  optim: paged_adamw_32bit\n",
      "  save_steps: 1000\n",
      "  logging_steps: 1000\n",
      "  learning_rate: 0.0002\n",
      "  weight_decay: 0.001\n",
      "  checkpoint_name: results\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load config params\n",
    "\n",
    "root_dir = get_root_dir()\n",
    "with initialize(config_path='config'):\n",
    "    cfg = compose(config_name='conf')\n",
    "    print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8e7a530-a02e-4661-a0af-a998aeb0f97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up quantization config only if LoRA variation is the specified PEFT method\n",
    "if 'abc' in cfg.peft.method:\n",
    "    # config quantization for QLoRA\n",
    "    print('configure quantization parameters')\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=cfg.model.load_in_4bit,\n",
    "        bnb_4bit_quant_type=cfg.model.quant_type,\n",
    "        bnb_4bit_compute_dtype=cfg.model.quant_compute_dtype,\n",
    "        bnb_4bit_use_double_quant=cfg.model.use_double_quant,\n",
    "    )\n",
    "    device_map = {\"\": 0}\n",
    "\n",
    "else:\n",
    "    bnb_config = None\n",
    "    device_map = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca867ed-c874-49b0-b1a6-3242f1cb3147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model bigscience/bloomz-560m\n"
     ]
    }
   ],
   "source": [
    "# load base model\n",
    "base_model_name = cfg.model.model_name\n",
    "print(f'load model {base_model_name}')\n",
    "foundation_model = AutoModelForCausalLM.from_pretrained(base_model_name,\n",
    "                                                        quantization_config=bnb_config,\n",
    "                                                        device_map=device_map)\n",
    "foundation_model.config.use_cache = False\n",
    "foundation_model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2df490d-15f1-4f49-99f0-8b33308685ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "# load data from s3 bucket\n",
    "data_location = f's3://{cfg.data.bucket}/{cfg.data.folder}'\n",
    "if cfg.data.datasets is not None:  # read only specified datasets\n",
    "    dfs = [pd.read_csv(f'{data_location}/{ds}.csv') for ds in cfg.data.datasets]\n",
    "else:  # read all available datasets\n",
    "    dfs = [pd.read_csv(f'{data_location}/{ds}') for ds in os.listdir(data_location)\n",
    "           if Path(f'{data_location}/{ds}.csv').exists() and Path(f'{data_location}/{ds}').suffix == '.csv']\n",
    "\n",
    "if len(dfs) > 1:\n",
    "    combined_df = pd.concat([df for df in dfs], ignore_index=True)\n",
    "combined_df = combined_df.dropna()  # Drop rows with missing values\n",
    "\n",
    "# tokenize all data\n",
    "inputs = tokenizer(combined_df['text'].tolist(), max_length=512, truncation=True,\n",
    "                   padding='max_length', return_tensors='pt')\n",
    "\n",
    "# transform datasets to pytorch Dataset instance\n",
    "train_ds = FootballTorchDataset(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d01f7a-0add-4f24-b629-3c84094c6af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config peft method and parameters\n",
    "method_name = cfg.peft.method\n",
    "\n",
    "# define peft methods and configurations\n",
    "if method_name == 'lora':\n",
    "    peft_config = LoraConfig(r=cfg.peft.r, lora_alpha=cfg.peft.lora_a,\n",
    "                             target_modules=cfg.peft.target_modules,\n",
    "                             lora_dropout=cfg.peft.dropout, bias='none')\n",
    "elif method_name == 'adalora':\n",
    "    peft_config = AdaLoraConfig(r=cfg.peft.r, lora_alpha=cfg.peft.lora_a,\n",
    "                                target_modules=cfg.peft.target_modules,\n",
    "                                lora_dropout=cfg.peft.dropout)\n",
    "elif method_name == 'lokr':\n",
    "    peft_config = LoKrConfig(r=cfg.peft.r, lora_alpha=cfg.peft.lora_a,\n",
    "                             target_modules=cfg.peft.target_modules)\n",
    "else:\n",
    "    peft_config = PrefixTuningConfig(num_virtual_tokens=20, token_dim=768,\n",
    "                                     num_transformer_submodules=1,\n",
    "                                     num_attention_heads=12, num_layers=12,\n",
    "                                     encoder_hidden_size=768)\n",
    "\n",
    "\n",
    "# define peft methods and configurations\n",
    "print(f'configure {cfg.peft.method} for PEFT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3e2f6f-32d8-47a7-b0ff-04ef6c6ae53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training arguments\n",
    "checkpoint_name = cfg.train.checkpoint_name\n",
    "out_dir = os.path.join(root_dir, checkpoint_name)\n",
    "Path(out_dir).mkdir(parents=True, exists_ok=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=out_dir,\n",
    "    num_train_epochs=cfg.train.num_epochs,\n",
    "    per_device_train_batch_size=cfg.train.batch_size,\n",
    "    gradient_accumulation_steps=cfg.train.gradient_accumulation_steps,\n",
    "    optim=cfg.train.optim,\n",
    "    save_steps=cfg.train.save_steps,\n",
    "    logging_steps=cfg.train.logging_steps,\n",
    "    learning_rate=cfg.train.learning_rate,\n",
    "    weight_decay=cfg.train.weight_decay,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type='constant',\n",
    "    report_to=\"tensorboard\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f964d235-2f3f-4f0b-a3d2-749dcbd9f814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "print('start training')\n",
    "print(\"=\" * 80)\n",
    "trainer = SFTTrainer(\n",
    "    model=foundation_model,\n",
    "    train_dataset=train_ds,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=None,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    packing=False\n",
    ")\n",
    "trainer.train()\n",
    "print(\"=\" * 80)\n",
    "print('save fine-tuned model')\n",
    "trainer.model.save_pretrained(f'{base_model_name}-football-{cfg.peft.method}-ft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8e75b3-a02d-43cb-aa85-47ff0fece72a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e463378-0a7b-4658-b476-af531165feb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cf83ff-bcbb-477b-9d7e-4d6c89bc5cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
